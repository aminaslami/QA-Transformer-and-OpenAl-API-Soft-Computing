{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAUKn3KEmmYr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import torch\n",
        "import csv\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import List, Dict, Tuple, Optional, Union\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmNGk5sH5K1w"
      },
      "outputs": [],
      "source": [
        "\"\"\"def set_openai_api_key(api_key: str) -> None:\n",
        "   def get_openai_api_key() -> str:\n",
        "\n",
        "    api_key = \"\"\n",
        "    config.openai_api_key = api_key\n",
        "    openai.api_key = api_key\n",
        "    print(\"API key set successfully.\")\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def get_openai_api_key(api_key: str) -> None:\n",
        "    client = OpenAI(api_key=\"Write Your OpenAI Key -> Here\")\n",
        "    #response = client.chat.completions.create(...)\n",
        "    print(\"Client = API key set successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "x0300g0BwvvQ"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.openai_api_key = \"\"  # Will be set by user\n",
        "        self.model_name = \"gpt-3.5-turbo\"\n",
        "        self.embedding_model = \"all-MiniLM-L6-v2\"\n",
        "        self.temperature = 0.2\n",
        "        self.max_tokens = 150\n",
        "        self.top_k = 5  # Number of most similar entries to consider\n",
        "        self.similarity_threshold = 0.6\n",
        "        self.csv_path = \"data.csv\"\n",
        "        self.question_col = \"question\"\n",
        "        self.answer_col = \"answer\"\n",
        "        self.log_file = f\"qa_logs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "        self.test_size = 0.2\n",
        "        self.few_shot_examples = 3  # Number of examples to include in prompt\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlrfRG7Oy36N"
      },
      "outputs": [],
      "source": [
        "def load_csv_data(file_path: str = None) -> pd.DataFrame:\n",
        "    file_path = input(\"/content/Firat-Uni-Fen-Bilimler-Enstitu-Soru-ve-Cevap-512-Rows.csv\") or config.csv_path\n",
        "    if file_path is None:\n",
        "        file_path = config.csv_path\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Loaded data with {len(df)} rows and {len(df.columns)} columns.\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "    return df\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame, question_col: str = None, answer_col: str = None) -> pd.DataFrame:\n",
        "    \"\"\"Preprocess the CSV data for Q&A tasks.\"\"\"\n",
        "    # Allow the user to select columns if not specified\n",
        "    if question_col is None or answer_col is None:\n",
        "        print(\"Available columns:\")\n",
        "        for i, col in enumerate(df.columns):\n",
        "            print(f\"{i}: {col}\")\n",
        "\n",
        "        if question_col is None:\n",
        "            q_idx = int(input(\"Enter the index of the question column: \"))\n",
        "            question_col = df.columns[q_idx]\n",
        "            config.question_col = question_col\n",
        "\n",
        "        if answer_col is None:\n",
        "            a_idx = int(input(\"Enter the index of the answer column: \"))\n",
        "            answer_col = df.columns[a_idx]\n",
        "            config.answer_col = answer_col\n",
        "\n",
        "    # Drop rows with NaN values in either column\n",
        "    df = df[[question_col, answer_col]].dropna()\n",
        "\n",
        "    # Convert to string type\n",
        "    df[question_col] = df[question_col].astype(str)\n",
        "    df[answer_col] = df[answer_col].astype(str)\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(subset=[question_col])\n",
        "\n",
        "    print(f\"After preprocessing: {len(df)} rows\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBE8Kph1zFNx"
      },
      "outputs": [],
      "source": [
        "class EmbeddingService:\n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"Initialize the embedding service with a pretrained model.\"\"\"\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.cache = {}\n",
        "\n",
        "    def get_embedding(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Get embedding for a text, using cache if available.\"\"\"\n",
        "        if text in self.cache:\n",
        "            return self.cache[text]\n",
        "\n",
        "        embedding = self.model.encode(text)\n",
        "        self.cache[text] = embedding\n",
        "        return embedding\n",
        "\n",
        "    def batch_embed(self, texts: List[str], batch_size: int = 32) -> List[np.ndarray]:\n",
        "        \"\"\"Generate embeddings for a list of texts in batches.\"\"\"\n",
        "        embeddings = []\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
        "            batch = texts[i:i+batch_size]\n",
        "\n",
        "            # Check which texts are not in cache\n",
        "            new_texts = [text for text in batch if text not in self.cache]\n",
        "            new_embeddings = self.model.encode(new_texts) if new_texts else []\n",
        "\n",
        "            # Add new embeddings to cache\n",
        "            for text, emb in zip(new_texts, new_embeddings):\n",
        "                self.cache[text] = emb\n",
        "\n",
        "            # Get all embeddings for the batch (from cache now)\n",
        "            batch_embeddings = [self.get_embedding(text) for text in batch]\n",
        "            embeddings.extend(batch_embeddings)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def find_similar(self, query: str, corpus: List[str], corpus_embeddings: List[np.ndarray]) -> List[Tuple[int, float]]:\n",
        "        \"\"\"Find most similar corpus items to the query.\"\"\"\n",
        "        query_embedding = self.get_embedding(query)\n",
        "\n",
        "        # Calculate cosine similarities\n",
        "        similarities = []\n",
        "        for i, emb in enumerate(corpus_embeddings):\n",
        "            sim = cosine_similarity([query_embedding], [emb])[0][0]\n",
        "            similarities.append((i, sim))\n",
        "\n",
        "        # Sort by similarity (descending)\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Filter by threshold\n",
        "        filtered = [(i, sim) for i, sim in similarities if sim >= config.similarity_threshold]\n",
        "\n",
        "        # Return top k results\n",
        "        return filtered[:config.top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0FudD8CzJ1W"
      },
      "outputs": [],
      "source": [
        "class OpenAIService:\n",
        "    def __init__(self, api_key: str, model_name: str = \"gpt-3.5-turbo\"):\n",
        "        \"\"\"Initialize the OpenAI service with API key and model name.\"\"\"\n",
        "        client = OpenAI(api_key=\"Write Your OpenAI Key -> Here\")\n",
        "        self.model = model_name\n",
        "        self.client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "    def call_api(self, messages: List[Dict], temperature: float = 0.2, max_tokens: int = 150) -> str:\n",
        "        \"\"\"Call OpenAI API with proper error handling and rate limiting.\"\"\"\n",
        "        retry_count = 0\n",
        "        max_retries = 3\n",
        "\n",
        "        while retry_count < max_retries:\n",
        "            try:\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=self.model,\n",
        "                    messages=messages,\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=max_tokens\n",
        "                )\n",
        "                return response.choices[0].message.content.strip()\n",
        "            except openai.RateLimitError:\n",
        "                wait_time = 2 ** retry_count\n",
        "                print(f\"Rate limit exceeded. Waiting {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "                retry_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error calling OpenAI API: {e}\")\n",
        "                time.sleep(1)\n",
        "                retry_count += 1\n",
        "\n",
        "        return \"I'm sorry, but I couldn't generate a response due to API errors.\"\n",
        "\n",
        "    def generate_answer(self, question: str, context: List[Dict[str, str]], few_shot_examples: List[Dict[str, str]] = None) -> str:\n",
        "        \"\"\"Generate an answer based on the question and retrieved context.\"\"\"\n",
        "        # Prepare context string\n",
        "        context_str = \"\\n\\n\".join([f\"Question: {item['question']}\\nAnswer: {item['answer']}\" for item in context])\n",
        "\n",
        "        # Prepare few-shot examples if provided\n",
        "        few_shot_str = \"\"\n",
        "        if few_shot_examples:\n",
        "            few_shot_str = \"\\n\\n\".join([f\"Question: {ex['question']}\\nAnswer: {ex['answer']}\" for ex in few_shot_examples])\n",
        "            few_shot_str = \"Here are some examples of how to answer questions:\\n\\n\" + few_shot_str + \"\\n\\n\"\n",
        "\n",
        "        # Construct messages\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": f\"You are an assistant that answers questions based on the provided context. Use the context information to formulate accurate, concise answers. If the context doesn't contain relevant information, say 'I don't have enough information to answer this question.'\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{few_shot_str}Context information:\\n{context_str}\\n\\nQuestion: {question}\\n\\nAnswer:\"}\n",
        "        ]\n",
        "\n",
        "        return self.call_api(messages, temperature=config.temperature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "uWfzrTFezXT2"
      },
      "outputs": [],
      "source": [
        "class TransferLearningQA:\n",
        "    def __init__(self, df: pd.DataFrame = None):\n",
        "        \"\"\"Initialize the QA system with data and services.\"\"\"\n",
        "        # Load data if not provided\n",
        "        if df is None:\n",
        "            df = load_csv_data(config.csv_path)\n",
        "            df = preprocess_data(df, config.question_col, config.answer_col)\n",
        "\n",
        "        self.df = df\n",
        "        self.question_col = config.question_col\n",
        "        self.answer_col = config.answer_col\n",
        "\n",
        "        # Initialize services\n",
        "        self.embedding_service = EmbeddingService(config.embedding_model)\n",
        "        self.openai_service = None  # Will be initialized when API key is provided\n",
        "\n",
        "        # Generate embeddings for all questions\n",
        "        print(\"Generating embeddings for all questions...\")\n",
        "        self.questions = df[self.question_col].tolist()\n",
        "        self.answers = df[self.answer_col].tolist()\n",
        "        self.question_embeddings = self.embedding_service.batch_embed(self.questions)\n",
        "\n",
        "        # Prepare train/test split for evaluation\n",
        "        self._prepare_train_test()\n",
        "\n",
        "        # Performance tracking\n",
        "        self.query_log = []\n",
        "\n",
        "    def _prepare_train_test(self):\n",
        "        \"\"\"Split data into train and test sets for evaluation.\"\"\"\n",
        "        indices = list(range(len(self.df)))\n",
        "        train_indices, test_indices = train_test_split(indices, test_size=config.test_size, random_state=42)\n",
        "\n",
        "        self.train_indices = train_indices\n",
        "        self.test_indices = test_indices\n",
        "\n",
        "        print(f\"Data split: {len(train_indices)} training examples, {len(test_indices)} test examples\")\n",
        "\n",
        "    def initialize_openai(self, api_key: str = None):\n",
        "        \"\"\"Initialize the OpenAI service with the provided API key.\"\"\"\n",
        "        if api_key is None:\n",
        "            api_key = config.openai_api_key\n",
        "            if not api_key:\n",
        "                api_key = input(\"Please enter your OpenAI API key: \")\n",
        "                config.openai_api_key = api_key\n",
        "\n",
        "        self.openai_service = OpenAIService(api_key, config.model_name)\n",
        "        print(f\"OpenAI service initialized with model: {config.model_name}\")\n",
        "\n",
        "    def get_few_shot_examples(self) -> List[Dict[str, str]]:\n",
        "        \"\"\"Get a few representative examples for few-shot learning.\"\"\"\n",
        "        # Select random examples from the training set\n",
        "        sample_indices = np.random.choice(self.train_indices, min(config.few_shot_examples, len(self.train_indices)), replace=False)\n",
        "\n",
        "        examples = []\n",
        "        for idx in sample_indices:\n",
        "            examples.append({\n",
        "                \"question\": self.questions[idx],\n",
        "                \"answer\": self.answers[idx]\n",
        "            })\n",
        "\n",
        "        return examples\n",
        "\n",
        "    def answer_question(self, question: str, use_few_shot: bool = True) -> Dict:\n",
        "        \"\"\"Answer a question using the QA pipeline.\"\"\"\n",
        "        # Initialize OpenAI service if not done already\n",
        "        if self.openai_service is None:\n",
        "            self.initialize_openai()\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Find similar questions\n",
        "        similar_indices = self.embedding_service.find_similar(\n",
        "            question,\n",
        "            self.questions,\n",
        "            self.question_embeddings\n",
        "        )\n",
        "\n",
        "        # Get few-shot examples if enabled\n",
        "        few_shot_examples = self.get_few_shot_examples() if use_few_shot else None\n",
        "\n",
        "        # Prepare context for the model\n",
        "        context = []\n",
        "        for idx, similarity in similar_indices:\n",
        "            context.append({\n",
        "                \"question\": self.questions[idx],\n",
        "                \"answer\": self.answers[idx],\n",
        "                \"similarity\": similarity\n",
        "            })\n",
        "\n",
        "        # Generate answer\n",
        "        answer = self.openai_service.generate_answer(\n",
        "            question,\n",
        "            context,\n",
        "            few_shot_examples\n",
        "        )\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        # Log this query\n",
        "        log_entry = {\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"context\": context,\n",
        "            \"time\": elapsed_time,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        self.query_log.append(log_entry)\n",
        "\n",
        "        # Log to file\n",
        "        with open(config.log_file, 'a') as f:\n",
        "            f.write(f\"Q: {question}\\n\")\n",
        "            f.write(f\"A: {answer}\\n\")\n",
        "            f.write(f\"Time: {elapsed_time:.2f}s\\n\")\n",
        "            f.write(\"-\" * 50 + \"\\n\")\n",
        "\n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"context\": context,\n",
        "            \"time\": elapsed_time\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeT9D_EVzcIr"
      },
      "outputs": [],
      "source": [
        "    def evaluate(self, num_samples: int = None) -> Dict:\n",
        "        \"\"\"Evaluate the QA system on test data.\"\"\"\n",
        "        if self.openai_service is None:\n",
        "            self.initialize_openai()\n",
        "\n",
        "        if num_samples is None:\n",
        "            num_samples = min(len(self.test_indices), 20)  # Default to 20 samples\n",
        "\n",
        "        # Sample questions from test set\n",
        "        sample_indices = np.random.choice(self.test_indices, num_samples, replace=False)\n",
        "\n",
        "        results = []\n",
        "        for idx in tqdm(sample_indices, desc=\"Evaluating\"):\n",
        "            question = self.questions[idx]\n",
        "            ground_truth = self.answers[idx]\n",
        "\n",
        "            result = self.answer_question(question)\n",
        "            result[\"ground_truth\"] = ground_truth\n",
        "            results.append(result)\n",
        "\n",
        "        # Calculate basic metrics\n",
        "        times = [r[\"time\"] for r in results]\n",
        "        avg_time = sum(times) / len(times)\n",
        "\n",
        "        return {\n",
        "            \"num_samples\": num_samples,\n",
        "            \"average_time\": avg_time,\n",
        "            \"details\": results\n",
        "        }\n",
        "\n",
        "    def show_statistics(self):\n",
        "        \"\"\"Display statistics about the QA system.\"\"\"\n",
        "        # Count queries\n",
        "        query_count = len(self.query_log)\n",
        "\n",
        "        if query_count == 0:\n",
        "            print(\"No queries have been processed yet.\")\n",
        "            return\n",
        "\n",
        "        # Calculate timing statistics\n",
        "        times = [log[\"time\"] for log in self.query_log]\n",
        "        avg_time = sum(times) / len(times)\n",
        "        max_time = max(times)\n",
        "        min_time = min(times)\n",
        "\n",
        "        # Get context info\n",
        "        context_sizes = [len(log[\"context\"]) for log in self.query_log]\n",
        "        avg_context_size = sum(context_sizes) / len(context_sizes)\n",
        "\n",
        "        # Print statistics\n",
        "        print(f\"Total queries: {query_count}\")\n",
        "        print(f\"Average processing time: {avg_time:.2f}s\")\n",
        "        print(f\"Min processing time: {min_time:.2f}s\")\n",
        "        print(f\"Max processing time: {max_time:.2f}s\")\n",
        "        print(f\"Average context size: {avg_context_size:.1f} entries\")\n",
        "\n",
        "        # Plot time distribution\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(times, bins=10)\n",
        "        plt.title(\"Query Processing Time Distribution\")\n",
        "        plt.xlabel(\"Time (seconds)\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e30417da21644d0296a71aafaef6e33e",
            "98af946265c04269adbaefbf1ed85660",
            "1d0622d2886e49b9842ac5e5636ab2f5",
            "4656b5924d38402eace4495f73d0a392",
            "a6731c1391ca47b292be939b784e15a3",
            "cc16f8f6db564732b257a7307afd4ca7",
            "8dd7912edadd4b6f9c0e9a01bf75af23",
            "622ac68ec4994f158454a0eb4c787eb2",
            "953d71e1ccf54d48831fd5d21a31c6c6",
            "0500476b41b440f780f78583e1110f34",
            "1faba1d4dcc34d7686790f5b901a90e1"
          ]
        },
        "id": "kai997wMz1NP",
        "outputId": "c293817f-054e-4dee-a179-f1661292f9e7"
      },
      "outputs": [],
      "source": [
        "def run_qa_system():\n",
        "    \"\"\"Run the transfer learning QA system with an existing CSV file.\"\"\"\n",
        "    print(\"Transfer Learning QA System Initialization\")\n",
        "    print(\"------------------------------------------\")\n",
        "\n",
        "    # Set API key\n",
        "    api_key = input(\"Write Your OpenAI Key -> Here\")\n",
        "    set_openai_api_key(api_key)\n",
        "\n",
        "    # Load data from existing CSV\n",
        "    file_path = input(\"/content/Firat-Uni-Fen-Bilimler-Enstitu-Soru-ve-Cevap-512-Rows.csv\")\n",
        "    df = load_csv_data(file_path)\n",
        "    df = preprocess_data(df)\n",
        "\n",
        "    # Initialize QA system\n",
        "    print(\"\\nInitializing QA system...\")\n",
        "    qa_system = TransferLearningQA(df)\n",
        "    qa_system.initialize_openai(api_key)\n",
        "\n",
        "    # Interactive Q&A loop\n",
        "    print(\"\\nQA System ready! Type 'quit' to exit, 'stats' to see statistics, or 'eval' to run evaluation.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nEnter your question: \")\n",
        "\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "        elif user_input.lower() == 'stats':\n",
        "            qa_system.show_statistics()\n",
        "        elif user_input.lower() == 'eval':\n",
        "            num_samples = int(input(\"How many samples to evaluate? (default: 10) \") or 10)\n",
        "            results = qa_system.evaluate(num_samples)\n",
        "            print(f\"Evaluation complete. Average processing time: {results['average_time']:.2f}s\")\n",
        "        else:\n",
        "            result = qa_system.answer_question(user_input)\n",
        "            print(f\"\\nAnswer: {result['answer']}\")\n",
        "            print(f\"\\nProcessing time: {result['time']:.2f}s\")\n",
        "\n",
        "            # Show context info if requested\n",
        "            show_context = input(\"Show retrieval context? (y/n): \")\n",
        "            if show_context.lower() == 'y':\n",
        "                print(\"\\nRetrieved context:\")\n",
        "                for i, ctx in enumerate(result['context']):\n",
        "                    print(f\"{i+1}. Q: {ctx['question']}\")\n",
        "                    print(f\"   A: {ctx['answer']}\")\n",
        "                    print(f\"   Similarity: {ctx['similarity']:.4f}\")\n",
        "\n",
        "    print(\"Thank you for using the Transfer Learning QA System!\")\n",
        "\n",
        "# ======================================================\n",
        "# 10. MAIN EXECUTION\n",
        "# ======================================================\n",
        "if __name__ == \"__main__\":\n",
        "    run_qa_system()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0500476b41b440f780f78583e1110f34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d0622d2886e49b9842ac5e5636ab2f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_622ac68ec4994f158454a0eb4c787eb2",
            "max": 16,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_953d71e1ccf54d48831fd5d21a31c6c6",
            "value": 16
          }
        },
        "1faba1d4dcc34d7686790f5b901a90e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4656b5924d38402eace4495f73d0a392": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0500476b41b440f780f78583e1110f34",
            "placeholder": "​",
            "style": "IPY_MODEL_1faba1d4dcc34d7686790f5b901a90e1",
            "value": " 16/16 [00:00&lt;00:00, 49.55it/s]"
          }
        },
        "622ac68ec4994f158454a0eb4c787eb2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dd7912edadd4b6f9c0e9a01bf75af23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "953d71e1ccf54d48831fd5d21a31c6c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98af946265c04269adbaefbf1ed85660": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc16f8f6db564732b257a7307afd4ca7",
            "placeholder": "​",
            "style": "IPY_MODEL_8dd7912edadd4b6f9c0e9a01bf75af23",
            "value": "Generating embeddings: 100%"
          }
        },
        "a6731c1391ca47b292be939b784e15a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc16f8f6db564732b257a7307afd4ca7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e30417da21644d0296a71aafaef6e33e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98af946265c04269adbaefbf1ed85660",
              "IPY_MODEL_1d0622d2886e49b9842ac5e5636ab2f5",
              "IPY_MODEL_4656b5924d38402eace4495f73d0a392"
            ],
            "layout": "IPY_MODEL_a6731c1391ca47b292be939b784e15a3"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
